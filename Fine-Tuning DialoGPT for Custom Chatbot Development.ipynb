{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Fine-Tuning DialoGPT for Custom Chatbot Development\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "This project demonstrates the fine-tuning of a pre-trained language model, **DialoGPT**, for conversational AI. By leveraging the Hugging Face `transformers` library, the model is adapted to a custom dataset to create a chatbot tailored to specific needs. The project includes data preparation, fine-tuning, and interaction with the chatbot.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose and Objectives**\n",
    "The primary goals of this project are:\n",
    "1. To fine-tune the pre-trained **DialoGPT** model on a custom conversational dataset.\n",
    "2. To create a conversational agent capable of generating context-aware responses.\n",
    "3. To implement an efficient pipeline for dataset tokenization, training, and deployment.\n",
    "4. To provide a reusable framework for further customization and enhancement.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components and Explanation**\n",
    "\n",
    "#### **1. Model and Tokenizer**\n",
    "- **Model Used:** `microsoft/DialoGPT-medium`, a pre-trained conversational model.\n",
    "- **Tokenizer:** Converts raw text into token IDs that the model can process. Ensures padding tokens (`pad_token`) are compatible with the modelâ€™s architecture.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Dataset Preparation**\n",
    "- **Custom Dataset:** A text file (`chat_dataset.txt`) containing conversational examples, with one dialogue turn per line.\n",
    "- **Tokenization:**\n",
    "  - Splits text into tokens of fixed length (`block_size=128`).\n",
    "  - Truncates or pads input to ensure uniformity across batches.\n",
    "- **Caching:** Writes intermediate results to a specified directory for efficient processing.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Data Collation**\n",
    "- A **Data Collator for Language Modeling** is used to handle dynamic padding during training. This ensures input sequences of varying lengths can be efficiently processed without modifying the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Training Configuration**\n",
    "- **TrainingArguments:** Specifies hyperparameters for the fine-tuning process, such as:\n",
    "  - Epochs: \\( 3 \\)\n",
    "  - Batch size: \\( 4 \\)\n",
    "  - Learning rate: \\( 5 \\times 10^{-5} \\)\n",
    "  - Checkpointing: Saves model weights periodically (`save_steps=500`).\n",
    "  - Logging: Tracks progress (`logging_steps=100`).\n",
    "\n",
    "- **Trainer API:** Simplifies model training by integrating dataset management, evaluation, and checkpointing.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Fine-Tuning Process**\n",
    "- The model is fine-tuned on the tokenized dataset to adapt it to the specific conversational patterns of the input data.\n",
    "- Fine-tuned weights and tokenizer configurations are saved locally for deployment.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Chatbot Interaction**\n",
    "- The chatbot generates responses using the fine-tuned model.\n",
    "- User inputs are tokenized and passed to the model for generation.\n",
    "- Responses are decoded back into human-readable text, skipping special tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation**\n",
    "\n",
    "#### **Steps**\n",
    "1. **Model and Tokenizer Loading**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:07:53.996593Z",
     "iopub.status.busy": "2024-12-23T21:07:53.996201Z",
     "iopub.status.idle": "2024-12-23T21:07:54.001252Z",
     "shell.execute_reply": "2024-12-23T21:07:54.000204Z",
     "shell.execute_reply.started": "2024-12-23T21:07:53.996567Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from transformers.data.datasets.language_modeling import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:08:22.614324Z",
     "iopub.status.busy": "2024-12-23T21:08:22.613737Z",
     "iopub.status.idle": "2024-12-23T21:08:25.551302Z",
     "shell.execute_reply": "2024-12-23T21:08:25.550347Z",
     "shell.execute_reply.started": "2024-12-23T21:08:22.614279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load Pre-trained Model and Tokenizer\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the tokenizer has a pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:13:49.203114Z",
     "iopub.status.busy": "2024-12-23T21:13:49.202697Z",
     "iopub.status.idle": "2024-12-23T21:13:50.043441Z",
     "shell.execute_reply": "2024-12-23T21:13:50.042294Z",
     "shell.execute_reply.started": "2024-12-23T21:13:49.203082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67ef9a8354248f58390f47a3143f2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b3d9c1021845858734e3ba272d69e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Dataset Preparation\n",
    "def prepare_dataset(file_path, tokenizer, block_size=128, cache_dir=\"/kaggle/working/cache\"):\n",
    "    \"\"\"Loads and tokenizes the dataset.\"\"\"\n",
    "    # Ensure the cache directory is writable\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size,\n",
    "        overwrite_cache=True,  # Disable caching or overwrite existing cache\n",
    "        cache_dir=cache_dir,   # Use writable cache directory\n",
    "    )\n",
    "    return dataset\n",
    "# Specify a writable cache directory\n",
    "dataset_path = \"/kaggle/input/chat-dataset/QA.txt\"\n",
    "train_dataset = prepare_dataset(dataset_path, tokenizer, cache_dir=\"/kaggle/working/cache\")\n",
    "\n",
    "# Load the dataset using the datasets library\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": dataset_path})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:14:38.210918Z",
     "iopub.status.busy": "2024-12-23T21:14:38.210583Z",
     "iopub.status.idle": "2024-12-23T21:14:38.215269Z",
     "shell.execute_reply": "2024-12-23T21:14:38.214197Z",
     "shell.execute_reply.started": "2024-12-23T21:14:38.210894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Data Collator for Padding\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:15:15.920081Z",
     "iopub.status.busy": "2024-12-23T21:15:15.919675Z",
     "iopub.status.idle": "2024-12-23T21:15:16.139326Z",
     "shell.execute_reply": "2024-12-23T21:15:16.138282Z",
     "shell.execute_reply.started": "2024-12-23T21:15:15.920051Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fine-tuning Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./chatbot_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:15:41.415657Z",
     "iopub.status.busy": "2024-12-23T21:15:41.415274Z",
     "iopub.status.idle": "2024-12-23T21:17:11.457017Z",
     "shell.execute_reply": "2024-12-23T21:17:11.456330Z",
     "shell.execute_reply.started": "2024-12-23T21:15:41.415624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=5.529878616333008, metrics={'train_runtime': 88.7893, 'train_samples_per_second': 0.27, 'train_steps_per_second': 0.068, 'total_flos': 5572204167168.0, 'train_loss': 5.529878616333008, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Fine-tune the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:18:05.241475Z",
     "iopub.status.busy": "2024-12-23T21:18:05.241070Z",
     "iopub.status.idle": "2024-12-23T21:18:08.325191Z",
     "shell.execute_reply": "2024-12-23T21:18:08.324036Z",
     "shell.execute_reply.started": "2024-12-23T21:18:05.241443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./chatbot_model/tokenizer_config.json',\n",
       " './chatbot_model/special_tokens_map.json',\n",
       " './chatbot_model/vocab.json',\n",
       " './chatbot_model/merges.txt',\n",
       " './chatbot_model/added_tokens.json',\n",
       " './chatbot_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./chatbot_model\")\n",
    "tokenizer.save_pretrained(\"./chatbot_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Chatbot Interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:18:33.145564Z",
     "iopub.status.busy": "2024-12-23T21:18:33.145232Z",
     "iopub.status.idle": "2024-12-23T21:18:33.150550Z",
     "shell.execute_reply": "2024-12-23T21:18:33.149534Z",
     "shell.execute_reply.started": "2024-12-23T21:18:33.145540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Interact with the Chatbot\n",
    "def chat_with_bot(prompt, model, tokenizer):\n",
    "    \"\"\"Generates a response for the given prompt.\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for chatting\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./chatbot_model\")\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./chatbot_model\")\n",
    "\n",
    "print(\"Chatbot ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_bot(user_input, fine_tuned_model, fine_tuned_tokenizer)\n",
    "    print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Sample Output**\n",
    "\n",
    "#### **Chat Example**\n",
    "```\n",
    "You: Hello! How are you?\n",
    "Bot: I'm just a chatbot, but I'm doing great! How about you?\n",
    "\n",
    "You: Tell me a joke.\n",
    "Bot: Why don't scientists trust atoms? Because they make up everything!\n",
    "\n",
    "You: exit\n",
    "Goodbye!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Customer Support:** Train on customer interaction logs to automate FAQs.\n",
    "2. **E-Learning:** Create conversational assistants for tutoring.\n",
    "3. **Healthcare:** Develop chatbots for symptom checks and guidance.\n",
    "4. **Entertainment:** Build conversational companions with specific personalities or themes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Future Enhancements**\n",
    "1. **Evaluation:** Add evaluation metrics (e.g., perplexity, BLEU) to assess model quality.\n",
    "2. **Dataset Augmentation:** Incorporate diverse conversational datasets for improved generalization.\n",
    "3. **Fine-Tuning Optimization:** Experiment with advanced techniques like LoRA or adapters for efficient fine-tuning.\n",
    "4. **Deployment:** Integrate the chatbot with a web or mobile interface for real-world use.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "This project demonstrates the fine-tuning of DialoGPT to create a custom chatbot capable of generating contextually relevant responses. The structured approach from data preparation to deployment ensures a robust conversational AI pipeline. Fine-tuning pre-trained models like DialoGPT is a powerful technique to adapt state-of-the-art NLP capabilities for specialized applications.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6356548,
     "sourceId": 10273273,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
